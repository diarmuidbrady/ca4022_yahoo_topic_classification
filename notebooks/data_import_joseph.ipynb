{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee38ba4-dc13-46b7-8f50-94f88b69dc52",
   "metadata": {},
   "source": [
    "# Yahoo Topic Classification\n",
    "\n",
    "Here we import the reduced data from Hive into pyspark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff719a6-166a-41bb-acf3-8be5d0f2409c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:38:08.745184Z",
     "iopub.status.busy": "2022-11-24T10:38:08.743460Z",
     "iopub.status.idle": "2022-11-24T10:38:09.328424Z",
     "shell.execute_reply": "2022-11-24T10:38:09.325422Z",
     "shell.execute_reply.started": "2022-11-24T10:38:08.744184Z"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb9a2b7-d4ec-4caf-bf1a-770986bfe332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T18:30:29.063028Z",
     "iopub.status.busy": "2022-11-25T18:30:29.063028Z",
     "iopub.status.idle": "2022-11-25T18:30:29.866542Z",
     "shell.execute_reply": "2022-11-25T18:30:29.865691Z",
     "shell.execute_reply.started": "2022-11-25T18:30:29.063028Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ac2de3-468f-403d-af60-dacbb291eadc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T18:30:29.868510Z",
     "iopub.status.busy": "2022-11-25T18:30:29.867511Z",
     "iopub.status.idle": "2022-11-25T18:30:29.896047Z",
     "shell.execute_reply": "2022-11-25T18:30:29.895506Z",
     "shell.execute_reply.started": "2022-11-25T18:30:29.868510Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/joluw/hadoop/spark-3.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b71490-54c7-4673-9b76-c01ec64d39ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T18:30:30.496136Z",
     "iopub.status.busy": "2022-11-25T18:30:30.495579Z",
     "iopub.status.idle": "2022-11-25T18:30:41.461272Z",
     "shell.execute_reply": "2022-11-25T18:30:41.460303Z",
     "shell.execute_reply.started": "2022-11-25T18:30:30.496136Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7dd9c6-2b0d-49f8-9bb5-a1b7d2f90f3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T15:12:38.695557Z",
     "iopub.status.busy": "2022-11-25T15:12:38.695557Z",
     "iopub.status.idle": "2022-11-25T15:12:42.365718Z",
     "shell.execute_reply": "2022-11-25T15:12:42.364754Z",
     "shell.execute_reply.started": "2022-11-25T15:12:38.695557Z"
    },
    "tags": []
   },
   "source": [
    "## Using Original tables\n",
    "\n",
    "Found that reducing the data using PySpark is more convenient than in Hive. When we used hive, the output table had delimeter issues which lead to parsing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f1f38e-baad-45de-92d4-a7cd72b23bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T18:34:07.412233Z",
     "iopub.status.busy": "2022-11-25T18:34:07.411236Z",
     "iopub.status.idle": "2022-11-25T18:34:13.127507Z",
     "shell.execute_reply": "2022-11-25T18:34:13.126507Z",
     "shell.execute_reply.started": "2022-11-25T18:34:07.412233Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test shapea: (60000, 4)\n"
     ]
    }
   ],
   "source": [
    "# import data into spark dataframe\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Google Drive URL wasn't working for the large train file, so uploaded datasets to data folder manually\n",
    "train = spark.read.csv(\"../data/train.csv\")\n",
    "test = spark.read.csv(\"../data/test.csv\")\n",
    "\n",
    "# rename columns\n",
    "train = train.select(f.col(\"_c0\").alias(\"topic\").astype('int'), f.col(\"_c1\").alias(\"q_title\"), f.col(\"_c2\").alias(\"q_content\"), f.col(\"_c3\").alias(\"answer\"))\n",
    "test = test.select(f.col(\"_c0\").alias(\"topic\").astype('int'), f.col(\"_c1\").alias(\"q_title\"), f.col(\"_c2\").alias(\"q_content\"), f.col(\"_c3\").alias(\"answer\"))\n",
    "\n",
    "print(\"test shapea:\", \"({}, {})\".format(test.count(), len(test.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "549d9273-3602-4d62-9eed-b5f1276c05d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T18:34:13.129508Z",
     "iopub.status.busy": "2022-11-25T18:34:13.128506Z",
     "iopub.status.idle": "2022-11-25T18:34:13.347707Z",
     "shell.execute_reply": "2022-11-25T18:34:13.346306Z",
     "shell.execute_reply.started": "2022-11-25T18:34:13.129508Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|topic|             q_title|           q_content|              answer|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    9|What makes friend...|How does the spar...|good communicatio...|\n",
      "|    2|Why does Zebras h...|What is the purpo...|this provides cam...|\n",
      "|    4|What did the itsy...|                null|          waterspout|\n",
      "|    4|What is the diffe...|                null|One difference be...|\n",
      "|    3|Why do women get ...|                null|Premenstrual synd...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f04dc7b-7b91-4690-90fa-1c4735b0e600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T18:34:13.348709Z",
     "iopub.status.busy": "2022-11-25T18:34:13.348709Z",
     "iopub.status.idle": "2022-11-25T18:34:15.702648Z",
     "shell.execute_reply": "2022-11-25T18:34:15.701651Z",
     "shell.execute_reply.started": "2022-11-25T18:34:13.348709Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/25 18:40:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/25 18:40:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/25 18:40:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/25 18:40:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|topic|percent|\n",
      "+-----+-------+\n",
      "|    1|    0.1|\n",
      "|    2|    0.1|\n",
      "|    3|    0.1|\n",
      "|    4|    0.1|\n",
      "|    5|    0.1|\n",
      "|    6|    0.1|\n",
      "|    7|    0.1|\n",
      "|    8|    0.1|\n",
      "|    9|    0.1|\n",
      "|   10|    0.1|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# reduce both datasets to quarter size using stratified sampling\n",
    "\n",
    "by_topic = train.groupby(\"topic\").count()\n",
    "by_topic = by_topic.withColumn(\"percent\", f.col(\"count\")/f.sum(\"count\").over(Window.partitionBy()))\n",
    "by_topic.orderBy('topic').select('topic', 'percent').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b225b31-ab63-4f5a-9f79-1f5848af1ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T18:35:42.719810Z",
     "iopub.status.busy": "2022-11-25T18:35:42.718808Z",
     "iopub.status.idle": "2022-11-25T18:35:45.912657Z",
     "shell.execute_reply": "2022-11-25T18:35:45.911657Z",
     "shell.execute_reply.started": "2022-11-25T18:35:42.719810Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (349667, 4)\n",
      "test shape: (14877, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/25 18:40:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/25 18:40:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 18:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|topic|round(percent, 2)|\n",
      "+-----+-----------------+\n",
      "|    1|              0.1|\n",
      "|    2|              0.1|\n",
      "|    3|              0.1|\n",
      "|    4|              0.1|\n",
      "|    5|              0.1|\n",
      "|    6|              0.1|\n",
      "|    7|              0.1|\n",
      "|    8|              0.1|\n",
      "|    9|              0.1|\n",
      "|   10|              0.1|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:===========================================>              (6 + 2) / 8]\r",
      "22/11/25 18:40:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/25 18:40:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df = train.sampleBy(col='topic', fractions={i:0.25 for i in range(1, 11, 1)})\n",
    "test_df = test.sampleBy(col='topic', fractions={i:0.25 for i in range(1, 11, 1)})\n",
    "\n",
    "print(\"train shape:\", \"({}, {})\".format(train_df.count(), len(train_df.columns)))\n",
    "print(\"test shape:\", \"({}, {})\".format(test_df.count(), len(test_df.columns)))\n",
    "\n",
    "\n",
    "by_topic = train_df.groupby(\"topic\").count()\n",
    "by_topic = by_topic.withColumn(\"percent\", f.col(\"count\")/f.sum(\"count\").over(Window.partitionBy()).astype('float'))\n",
    "#by_topic = by_topic.select(\"topic\", f.col(\"percent\").astype('float'))\n",
    "by_topic.orderBy('topic').select('topic', f.round('percent', 2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4342a461-60b0-4b89-8045-3e1f398e07f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T18:36:10.214889Z",
     "iopub.status.busy": "2022-11-25T18:36:10.214247Z",
     "iopub.status.idle": "2022-11-25T18:36:10.381118Z",
     "shell.execute_reply": "2022-11-25T18:36:10.379975Z",
     "shell.execute_reply.started": "2022-11-25T18:36:10.214889Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the reduced data\n",
    "train_df.write.csv(\"../data/train_reduced\")\n",
    "test_df.write.csv(\"../data/test_reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d1de5-7d8e-4949-8435-fa8150fa3ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
